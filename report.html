<!DOCTYPE html>
<html>
<head>
<title>report.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: "Segoe WPC", "Segoe UI", "SFUIText-Light", "HelveticaNeue-Light", sans-serif, "Droid Sans Fallback";
	font-size: 14px;
	padding: 0 12px;
	line-height: 22px;
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}


body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	color: #4080D0;
	text-decoration: none;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

h1 code,
h2 code,
h3 code,
h4 code,
h5 code,
h6 code {
	font-size: inherit;
	line-height: auto;
}

a:hover {
	color: #4080D0;
	text-decoration: underline;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left: 5px solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 14px;
	line-height: 19px;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

.mac code {
	font-size: 12px;
	line-height: 18px;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

/** Theming */

.vscode-light,
.vscode-light pre code {
	color: rgb(30, 30, 30);
}

.vscode-dark,
.vscode-dark pre code {
	color: #DDD;
}

.vscode-high-contrast,
.vscode-high-contrast pre code {
	color: white;
}

.vscode-light code {
	color: #A31515;
}

.vscode-dark code {
	color: #D7BA7D;
}

.vscode-light pre:not(.hljs),
.vscode-light code > div {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre:not(.hljs),
.vscode-dark code > div {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre:not(.hljs),
.vscode-high-contrast code > div {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

.vscode-light blockquote,
.vscode-dark blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.vscode-high-contrast blockquote {
	background: transparent;
	border-color: #fff;
}
</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family:  "Meiryo", "Segoe WPC", "Segoe UI", "SFUIText-Light", "HelveticaNeue-Light", sans-serif, "Droid Sans Fallback";
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>
<link rel="stylesheet" href="file:///Users/ozanpekmezci/Documents/capstone/report.css" type="text/css">
</head>
<body>
<h1 id="machine-learning-engineer-nanodegree">Machine Learning Engineer Nanodegree</h1>
<h2 id="capstone-project">Capstone Project</h2>
<p>Ozan Pekmezci
May 18th, 2018</p>
<h2 id="i-definition">I. Definition</h2>
<h3 id="approx-1-2-pages">(approx. 1-2 pages)</h3>
<h3 id="project-overview">Project Overview</h3>
<p>The aim of this project at hand is to build a software to detect house numbers on streets. The project was featured in the Deep Learning course of Udacity.</p>
<p>The domain is number recognition on videos. The app recognizes the numbers on the live image and shows it to the user. This project used <a href="http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42241.pdf">Google's paper</a> as a reference point. The paper explains Google's way to recognize multi-digit numbers from static Street View images using Deep Convolutional Neural Networks. This project also does the same using a different architecture. The best part of this project is the combination of Machine Learning with Software Engineering which are the field of interests of the author.</p>
<p>The project was split into three steps. The first being the digit recognition using synthetic dataset. Second one was using doing the same with real street number data and third the Android app implementation, which was optional on Udacity Deep Learning course. For the first step, MNIST dataset was used. MNIST is database that contains handwritten digits. Therefore, they are actually not the best to detect multi-digit street numbers. That's why the digits from MNIST were concatenated to simulate house numbers on streets. The second step uses SVHN dataset, which contains house numbers images acquired from Google Street View. Lastly, third step also was supposed to SVHN dataset, but it didn't came to life during the scope of this capstone due to the reason that Tensorflow Apps never ran on my phone.</p>
<h3 id="problem-statement">Problem Statement</h3>
<p>The problem is the fact that house numbers have different formats. The numbers can appear with non-standard baseline, broken outlines, non-standard fonts or bad localization. The goal was recognizing all of those cases.</p>
<p>The strategy to solve this problem is using Convolutional Neural Networks with Tensorflow framework. The end solution runs on Android operating system to increase portability. MNIST and SVHN datasets are used to train and test data. The algorithm receives images as an input and extracts digits from them if there are any.</p>
<h3 id="metrics">Metrics</h3>
<p>The metrics are coverage, overall accuracy and per character accuracy. In the first phase of the project, we achieved 91.77% overall accuracy and 98.24% per character accuracy. For coverage, we define a confidance threshold and discard the predictions that are less likelier than the threshold. Coverage is the proportion of non-discarded values to all values.</p>
<h2 id="ii-analysis">II. Analysis</h2>
<h3 id="approx-2-4-pages">(approx. 2-4 pages)</h3>
<h3 id="data-exploration">Data Exploration</h3>
<p>The main dataset to be used for this type of a problem is the SVHN dataset, which contains Google Street View House Numbers data, however the author chose MNIST database for the beginning. The reason is simple, MNIST database provides handwritten numbers and SVHN contains sequence of digits. That's why the initial idea was concatenating MNIST characters to form an artificial dataset so that we can avoid problematic situtations that occur on house numbers like digits being upside-down, containing some lines inbetween or written in another artistic way.</p>
<p>According to its <a href="http://yann.lecun.com/exdb/mnist/">official website</a>, MNIST dataset contains 60,000 training and 10,000 testing examples. All digits are normalized centered in a fixed-size image, which makes it a good choice for machine learning since it handles pre-processing steps for you. Another reason to use MNIST initially is the fact that, it is easy to import via Keras. Keras is a machine learning frontend that serves as an abstraction layer to run different machine learning backends like Tensorflow. It also is really easy to import MNIST with the line:</p>
<p><code>from keras.datasets import mnist</code></p>
<p>.After that, The dataset is minimal and has the size of 11 Megabytes. Each of the images are basically 28 by 28 pixels. Although they preserve color values, the software at hand transforms the images to black and white and uses them like that. This creates no problems, since different colors don't change the ability to recognize different digits on the images.</p>
<p>Example data point from MNIST:
<img src="./images/mnist_single.png" alt="alt text" title="Example data point from MNIST"></p>
<p>For the second stage of the capstone, a synthetic MNIST dataset is generated. Since 99.99% of the SVHN dataset contains house number length less than 5, the maximum length of the synthetic dataset set to be 5. This means that MNIST data points are stitched together to become data points with the length between 1 and 5. To do that, the blank character is utilized with the label 10. For example, this example has the label (7, 3, 1, 1 3).</p>
<p>Example from multi-digit MNIST:
<img src="./images/mnist_multi.png" alt="alt text" title="Example data point from multi digit MNIST"></p>
<p>The SVHN dataset on the other hand, is much <a href="http://ufldl.stanford.edu/housenumbers/">bigger</a>, has the size more than 200 megabytes. It contains 73257 training and 26032 testing examples. Those examples are directly extracted from Google Maps Street View, that's why all data are found in their real environment. By default, there are 10 classes, 1 for each digit. Digit '1' has label 1, '9' has label 9 and '0' has label 10. However, in our case, digit '0' has the label 0 and label 10 corresponds to the blank character. That's why a preprocessing is required. SVHN dataset comes in two different formats; format 1 contains original images with bounding boxes around characters. Format 2 has MNIST-like 32-by-32 images centered around a single character, which we used so that we can use similar model architectures for different versions. The original character bounding boxes are extended in the appropriate dimension to become square windows, so that resizing them to 32-by-32 pixels does not introduce aspect ratio distortions. However, getting and importing is as not easy as importing MNIST data since the dataset is provided in <code>.mat</code> format. The dataset should be downloaded and the data should be extracted from the dataset programatically.</p>
<p>Example data from SVHN:
<img src="./images/svhn_website.png" alt="alt text" title="Example data from SVHN"></p>
<h3 id="exploratory-visualization">Exploratory Visualization</h3>
<h4 id="amount-of-labels-on-mnist-dataset">Amount of labels on MNIST dataset</h4>
<p><img src="./images/mnist_analysis_single.png" alt="alt text" title="Amount of labels on MNIST dataset"></p>
<p>MNIST dataset contains examples of each digits in a fairly balanced way. The label 1 seems more than others and label 5 seems to be a bit less than others.</p>
<h4 id="amount-of-labels-on-synthetic-mnist-dataset">Amount of labels on Synthetic MNIST dataset</h4>
<p><img src="./images/mnist_analysis.png" alt="alt text" title="Amount of labels on Synthetic MNIST dataset"></p>
<p>Generated MNIST dataset contains digits with the maximum length of 5. Since they were set randomly, the distribution stayed the same as on single digit MNIST.</p>
<h4 id="amount-of-labels-on-svhn-dataset">Amount of labels on SVHN dataset</h4>
<p><img src="./images/svhn_analysis.png" alt="alt text" title="Amount of labels on SVHN dataset"></p>
<p>SVHN dataset looks like left-skewed bell curve that has the most examples of the label 1. The occurances of labels gets less and less starting from the label 2. Labels 0 and 9 seems to be the lowest for house numbers.</p>
<h3 id="algorithms-and-techniques">Algorithms and Techniques</h3>
<p>For the problem at hand, the author used Convolutional Neural Networks to predict digits from the images. Convolutional Neural Networks are ideal for image recognition, since they don't flatten the nodes, which removes the logic out of images.</p>
<p>Another software that the author would normally use was developing an Android application but there were compilation problems that couldn't be fixed for months, which moved that part to the backlog, which will be developed after this nanodegree ends.</p>
<h3 id="benchmark">Benchmark</h3>
<p>As benchmark we use the model that is specified in Google's paper. Image as input, hidden layers and an output layer that contains nodes that represent a digit each. The paper also mentions benchmark values for accuracy.</p>
<p>These benchmark values are coverage, overall accuracy and per character accuracy. The authors of the paper achieved 96.5% coverage, 96% overall accuracy and 97.8% per character accuracy. For coverage, we define a confidance threshold and discard the predictions that are less likelier than the threshold. Coverage is the proportion of non-discarded values to all values.</p>
<p>The model that was developed during the scope of this project, achieved the overall accuracy of 87.2% and per character accuracy of 96.8%. When the confidence threshold 70% was chosen, the coverage was 96.5%. However, the confidence threshold being 100% resulted the coverage being 46.7%. Therefore, algorithm is only 100% sure about the results of half of the data.</p>
<h2 id="iii-methodology">III. Methodology</h2>
<h3 id="approx-3-5-pages">(approx. 3-5 pages)</h3>
<p>grey: epoch: 12: 44.5, 24: 48, 48: 49.2, batch: 32: 86 - 52
adam, learningrate 0.001, 6 conv layers, batchnorm: 25%</p>
<h3 id="data-preprocessing">Data Preprocessing</h3>
<p>Data preprocessing is different for different steps of the project. For the first step, which is using MNIST for single digit recognition, the data is reshaped based on the image data format of the Keras instance, images are turned into gray to reduce complexity, then the RGB values values are normalised to be in the range 0 to 1 that is always beneficial for machine learning algorithms. Lastly, the labels are one-hot-encoded from class vector to binary class matrices, which again is required for machine learning algorithms to function well.</p>
<p>The second step is generating synthetic MNIST data and building image recognition model up to 5 consecutive digits. There, the author did what he did on the first step, plus a synthetic dataset is built. To do that, first a random length for each data is selected. Then, random indices are selected for each element. Next, the images and labels are stitched together to resemble actual data points and images. Lastly, blank images and their labels 10 are added to the required locations followed by resizing the resulting images to their right size, which is 64 to 64.</p>
<p>Last step was doing the same with the SVHN dataset. However, SVHN dataset is much harder to import than MNIST. That's why it needs more steps. First, the dataset gets downloaded, unpacked and extracted. We use <code>h5py</code> to import the  <code>.mat</code> files, so that we can reach file contents for each digit, like the position of the boxes around digits, label and the file name. After that, we use the the information about the box to crop the parts outside to remove irrelevant sections of the images. Then the concrete training and testing data points are acquired. There are 33402 data points, labels in the training set and 13068 in the training set. Only one of them in training set has the digit length more than 5.</p>
<p><img src="./images/svhn_removed.png" alt="svhn_removed" title="Removed element of SVHN dataset that contains 6 digits">
<em>Removed element of SVHN dataset that</em></p>
<p>In this section, all of your preprocessing steps will need to be clearly documented, if any were necessary. From the previous section, any of the abnormalities or characteristics that you identified about the dataset will be addressed and corrected here. Questions to ask yourself when writing this section:</p>
<ul>
<li><em>If the algorithms chosen require preprocessing steps like feature selection or feature transformations, have they been properly documented?</em></li>
<li><em>Based on the <strong>Data Exploration</strong> section, if there were abnormalities or characteristics that needed to be addressed, have they been properly corrected?</em></li>
<li><em>If no preprocessing is needed, has it been made clear why?</em></li>
</ul>
<h3 id="implementation">Implementation</h3>
<p>In this section, the process for which metrics, algorithms, and techniques that you implemented for the given data will need to be clearly documented. It should be abundantly clear how the implementation was carried out, and discussion should be made regarding any complications that occurred during this process. Questions to ask yourself when writing this section:</p>
<ul>
<li><em>Is it made clear how the algorithms and techniques were implemented with the given datasets or input data?</em></li>
<li><em>Were there any complications with the original metrics or techniques that required changing prior to acquiring a solution?</em></li>
<li><em>Was there any part of the coding process (e.g., writing complicated functions) that should be documented?</em></li>
</ul>
<h3 id="refinement">Refinement</h3>
<p>In this section, you will need to discuss the process of improvement you made upon the algorithms and techniques you used in your implementation. For example, adjusting parameters for certain models to acquire improved solutions would fall under the refinement category. Your initial and final solutions should be reported, as well as any significant intermediate results as necessary. Questions to ask yourself when writing this section:</p>
<ul>
<li><em>Has an initial solution been found and clearly reported?</em></li>
<li><em>Is the process of improvement clearly documented, such as what techniques were used?</em></li>
<li><em>Are intermediate and final solutions clearly reported as the process is improved?</em></li>
</ul>
<h2 id="iv-results">IV. Results</h2>
<h3 id="approx-2-3-pages">(approx. 2-3 pages)</h3>
<h3 id="model-evaluation-and-validation">Model Evaluation and Validation</h3>
<p>In this section, the final model and any supporting qualities should be evaluated in detail. It should be clear how the final model was derived and why this model was chosen. In addition, some type of analysis should be used to validate the robustness of this model and its solution, such as manipulating the input data or environment to see how the model’s solution is affected (this is called sensitivity analysis). Questions to ask yourself when writing this section:</p>
<ul>
<li><em>Is the final model reasonable and aligning with solution expectations? Are the final parameters of the model appropriate?</em></li>
<li><em>Has the final model been tested with various inputs to evaluate whether the model generalizes well to unseen data?</em></li>
<li><em>Is the model robust enough for the problem? Do small perturbations (changes) in training data or the input space greatly affect the results?</em></li>
<li><em>Can results found from the model be trusted?</em></li>
</ul>
<h3 id="justification">Justification</h3>
<p>In this section, your model’s final solution and its results should be compared to the benchmark you established earlier in the project using some type of statistical analysis. You should also justify whether these results and the solution are significant enough to have solved the problem posed in the project. Questions to ask yourself when writing this section:</p>
<ul>
<li><em>Are the final results found stronger than the benchmark result reported earlier?</em></li>
<li><em>Have you thoroughly analyzed and discussed the final solution?</em></li>
<li><em>Is the final solution significant enough to have solved the problem?</em></li>
</ul>
<h2 id="v-conclusion">V. Conclusion</h2>
<h3 id="approx-1-2-pages">(approx 1-2 pages)</h3>
<h3 id="free-form-visualization">Free-Form Visualization</h3>
<p>In this section, you will need to provide some form of visualization that emphasizes an important quality about the project. It is much more free-form, but should reasonably support a significant result or characteristic about the problem that you want to discuss. Questions to ask yourself when writing this section:</p>
<ul>
<li><em>Have you visualized a relevant or important quality about the problem, dataset, input data, or results?</em></li>
<li><em>Is the visualization thoroughly analyzed and discussed?</em></li>
<li><em>If a plot is provided, are the axes, title, and datum clearly defined?</em></li>
</ul>
<h3 id="reflection">Reflection</h3>
<p>In this section, you will summarize the entire end-to-end problem solution and discuss one or two particular aspects of the project you found interesting or difficult. You are expected to reflect on the project as a whole to show that you have a firm understanding of the entire process employed in your work. Questions to ask yourself when writing this section:</p>
<ul>
<li><em>Have you thoroughly summarized the entire process you used for this project?</em></li>
<li><em>Were there any interesting aspects of the project?</em></li>
<li><em>Were there any difficult aspects of the project?</em></li>
<li><em>Does the final model and solution fit your expectations for the problem, and should it be used in a general setting to solve these types of problems?</em></li>
</ul>
<h3 id="improvement">Improvement</h3>
<p>In this section, you will need to provide discussion as to how one aspect of the implementation you designed could be improved. As an example, consider ways your implementation can be made more general, and what would need to be modified. You do not need to make this improvement, but the potential solutions resulting from these changes are considered and compared/contrasted to your current solution. Questions to ask yourself when writing this section:</p>
<ul>
<li><em>Are there further improvements that could be made on the algorithms or techniques you used in this project?</em></li>
<li><em>Were there algorithms or techniques you researched that you did not know how to implement, but would consider using if you knew how?</em></li>
<li><em>If you used your final solution as the new benchmark, do you think an even better solution exists?</em></li>
</ul>
<hr>
<p><strong>Before submitting, ask yourself. . .</strong></p>
<ul>
<li>Does the project report you’ve written follow a well-organized structure similar to that of the project template?</li>
<li>Is each section (particularly <strong>Analysis</strong> and <strong>Methodology</strong>) written in a clear, concise and specific fashion? Are there any ambiguous terms or phrases that need clarification?</li>
<li>Would the intended audience of your project be able to understand your analysis, methods, and results?</li>
<li>Have you properly proof-read your project report to assure there are minimal grammatical and spelling mistakes?</li>
<li>Are all the resources used for this project correctly cited and referenced?</li>
<li>Is the code that implements your solution easily readable and properly commented?</li>
<li>Does the code execute without error and produce results similar to those reported?</li>
</ul>

</body>
</html>
